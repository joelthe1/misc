model_args: # model construction args
  src_vocab: 25377
  tgt_vocab: 27463
  enc_layers: 9
  dec_layers: 6
  hid_size: 768
  ff_size: 2048
  n_heads: 12
  attn_bias: true
  attn_dropout: 0.15
  dropout: 0.3
  activation: gelu
  tied_emb: one-way
model_type: tfmnmt  # model type. tfmnmt is the transformer NMT model
optim:
  args:
    betas:
    - 0.9
    - 0.98
    eps: 1.0e-09
    label_smoothing: 0.2
    lr: 0.0005
    warmup_steps: 100
    criterion: smooth_kld
    constant: 1
    amsgrad: true
    weight_decay: 0
    inv_sqrt: true
  name: ADAM
parent:
  experiment: /mnt/data/many-to-eng/models/rtg500eng-tfm9L6L768d-bsz720k-stp200k-ens05
  vocab:
    #shared: shared       # for reusing the shared vocab
    src: src            # for separate vocabs
    tgt: tgt
  shrink: true
  model:
    args: true          # update/overwrite the model_args of child with the parent
    ensemble: 1

prep: # data preparation
  codec_lib: nlcodec
  char_coverage: 0.9999
  max_src_types: 8000  # maximum number of types in vocab ; if shared_vocab=false, set max_src_types and max_tgt_types separately instead of this one
  max_tgt_types: 8000
  pieces: bpe   # choices: bpe, char, word, unigram  from google/sentencepiece
  shared_vocab: false  # true means same vocab for src and tgt, false means different vocabs
  src_len: 256   # longer sentences, decision is made as per 'truncate={true,false}'
  tgt_len: 256
  truncate: true   # what to do with longer
  sentences: if true truncate at src_len or tgt_len; if false filter away
  train_src: /mnt/data/many-to-eng/data/MT/experiments/mdhUV-mdhUBT/train.src.tok
  train_tgt: /mnt/data/many-to-eng/data/MT/experiments/mdhUV-mdhUBT/train.trg.tok
  valid_src: /mnt/data/many-to-eng/data/MT/experiments/mdhUV-mdhUBT/val.src.tok
  valid_tgt: /mnt/data/many-to-eng/data/MT/experiments/mdhUV-mdhUBT/val.trg.tok
tester:
  decoder:
    beam_size: 4
    batch_size: 12000  # this is for 1 beam; effective_batch_size = batch_size / beam_size
    lp_alpha: 0.6     # length penalty
    ensemble: 5
    max_len: 50
  suit:  # suit of tests to run after the training
    # name of test and list of src.tok, ref files (ref should be unmodified)
    dev:
    - /mnt/data/many-to-eng/data/MT/experiments/mdhUV-mdhUBT/val.src.tok
    - /mnt/data/many-to-eng/data/MT/experiments/mdhUV-mdhUBT/val.trg.txt
    test:
    - /mnt/data/many-to-eng/data/MT/experiments/mdhUV-mdhUBT/test.src.tok
    - /mnt/data/many-to-eng/data/MT/experiments/mdhUV-mdhUBT/test.trg.txt
trainer:
  init_args:
    chunk_size: 10
  batch_size: 2000
  check_point: 1000  # how often to checkpoint?
  keep_models: 10   # how many checkpoints to keep on disk (small enough to save disk, large enough for checkpt averaging
  steps: 200000   # how many steps to train
  early_stop:       # remove this block to disable
    enabled: true   # or, alternatively flip this to disable;
    by: loss        # stop by validation loss (default); TODO: add BLEU
    patience: 5    # how many validations to wait, to be sure of stopping; each validation is per check_point steps
    min_steps: 4000  # minimum steps to wait before test for early stop;
    signi_round: 4
updated_at: '2021-03-31T08:49:04.338407'
seed: 12345  # fix the manual seed of pytorch + cuda + numpy + python_stdlib RNGs.  Remove/comment this to disable
